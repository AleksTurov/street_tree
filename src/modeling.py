import torch
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix
from src.utils import logger
import numpy as np
from focal_loss import FocalLoss

class DeepLearningModel:
    """
    üìä –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã:

    –ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º Multilayer Perceptron (MLP) ‚Äî –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—É—é –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

    1. –•–∞—Ä–∞–∫—Ç–µ—Ä –¥–∞–Ω–Ω—ã—Ö:
        - –î–∞–Ω–Ω—ã–µ –æ –¥–µ—Ä–µ–≤—å—è—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, tree_dbh, borough, curb_loc) ‚Äî —Ç–∞–±–ª–∏—á–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.
        - MLP —Ö–æ—Ä–æ—à–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å —Ç–∞–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –∫–æ–≥–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º—ã –∏ –Ω–µ –∏–º–µ—é—Ç –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.

    2. –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–¥–∞—á–∏:
        - –≠—Ç–æ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (Good, Fair, Poor).
        - MLP —Å CrossEntropyLoss –∏ LogSoftmax –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á.

    üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:
    - –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (input_size): –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞–≤–Ω–∞ —á–∏—Å–ª—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.
    - –°–∫—Ä—ã—Ç—ã–µ —Å–ª–æ–∏:
        - 256, 128, 64 ‚Äì –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π –≥–ª—É–±–∏–Ω—ã.
        - BatchNorm ‚Äì –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –≤—ã—Ö–æ–¥—ã –∏ —É—Å–∫–æ—Ä—è–µ—Ç —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å.
        - Dropout (0.3) ‚Äì –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.
    - –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π:
        - LogSoftmax(dim=1) ‚Äì –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—ã—Ö–æ–¥—ã –≤ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

    üìà –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è:
    - Class Weights ‚Äì —É—á–∏—Ç—ã–≤–∞–µ—Ç –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (CrossEntropyLoss).
    - Adam Optimizer ‚Äì –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å learning_rate=0.0005.
    - ReduceLROnPlateau ‚Äì —Å–Ω–∏–∂–∞–µ—Ç learning_rate, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —É–ª—É—á—à–∞—Ç—å—Å—è (–Ω–∞ val_loss).
    """

    def __init__(self, input_size, hidden_layers, output_size=3, learning_rate=0.0005, model_dir="models", y_train=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.model = self._build_model(input_size, hidden_layers, output_size).to(self.device)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏
        if y_train is not None:
            y_train = np.array(y_train, dtype=int)
            if len(y_train) > 0 and len(np.unique(y_train)) > 1:
                class_counts = np.bincount(y_train)
                class_weights = 1.0 / class_counts
                weights = torch.tensor(class_weights, dtype=torch.float32).to(self.device)
                logger.info(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights}")
            else:
                logger.error("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –≤–µ—Å–æ–≤.")
                weights = None
        else:
            weights = None
            logger.error("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç y_train: –ø—Ä–æ–≤–µ—Ä—å —Ç–∏–ø –∏ —Ñ–æ—Ä–º—É.")

    
        self.criterion = FocalLoss(alpha=class_weights, gamma=2)
        self.optimizer = optim.AdamW(self.model.parameters(), lr=learning_rate)

        # –î–æ–±–∞–≤–ª—è–µ–º scheduler –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è learning rate
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=5)

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        self.model_dir = model_dir
        os.makedirs(self.model_dir, exist_ok=True)

    def _build_model(self, input_size, hidden_layers, output_size):
        layers = [nn.Linear(input_size, hidden_layers[0]), nn.BatchNorm1d(hidden_layers[0]), nn.ReLU(), nn.Dropout(0.3)]
        for in_size, out_size in zip(hidden_layers[:-1], hidden_layers[1:]):
            layers.append(nn.Linear(in_size, out_size))
            layers.append(nn.BatchNorm1d(out_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.4))
        layers.append(nn.Linear(hidden_layers[-1], output_size))
        layers.append(nn.LogSoftmax(dim=1))
        return nn.Sequential(*layers)

    def _prepare_data(self, X, y):
        X_tensor = torch.tensor(X.values, dtype=torch.float32).to(self.device)
        y_tensor = torch.tensor(y.values, dtype=torch.long).to(self.device)
        return TensorDataset(X_tensor, y_tensor)

    def train(self, X_train, y_train, X_val, y_val, num_epochs=100, batch_size=64):
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è")

        train_dataset = self._prepare_data(X_train, y_train)
        val_dataset = self._prepare_data(X_val, y_val)

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        best_val_loss = float('inf')

        for epoch in range(num_epochs):
            train_loss = self._train_one_epoch(train_loader)
            val_loss, metrics = self._validate(val_loader)

            logger.info(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{num_epochs}, –ü–æ—Ç–µ—Ä—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {train_loss:.4f}, –ü–æ—Ç–µ—Ä—è –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_loss:.4f}")
            logger.info(f"Accuracy: {metrics['accuracy']:.4f}, F1-Score: {metrics['f1_score']:.4f}, AUC-ROC: {metrics['auc_roc']:.4f}")
            logger.info(f"Confusion Matrix:\n{metrics['conf_matrix']}")

            self.scheduler.step(val_loss)

            if val_loss < best_val_loss:
                best_val_loss = val_loss
                self._save_model("best_model.pth")

        logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")

    def _train_one_epoch(self, train_loader):
        self.model.train()
        total_loss = 0

        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)

            self.optimizer.zero_grad()
            outputs = self.model(X_batch)
            loss = self.criterion(outputs, y_batch)

            loss.backward()
            self.optimizer.step()

            total_loss += loss.item()

        return total_loss / len(train_loader)

    def _validate(self, val_loader):
        self.model.eval()
        total_val_loss = 0

        all_preds = []
        all_probs = []
        all_labels = []

        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)

                outputs = self.model(X_batch)
                loss = self.criterion(outputs, y_batch)

                total_val_loss += loss.item()

                probs = torch.exp(outputs).cpu().numpy()
                preds = np.argmax(probs, axis=1)

                all_probs.extend(probs)
                all_preds.extend(preds)
                all_labels.extend(y_batch.cpu().numpy())

        metrics = {
            'accuracy': accuracy_score(all_labels, all_preds),
            'f1_score': f1_score(all_labels, all_preds, average='weighted'),
            'auc_roc': roc_auc_score(all_labels, np.array(all_probs), multi_class='ovr'),
            'conf_matrix': confusion_matrix(all_labels, all_preds)
        }

        return total_val_loss / len(val_loader), metrics

    def _save_model(self, filename):
        path = os.path.join(self.model_dir, filename)
        torch.save(self.model.state_dict(), path)
        logger.info(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤ {path}")